{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "18fTJ7lJp_3U-cBcFMRXWTk8DRiN-SrnM",
      "authorship_tag": "ABX9TyPoKtvaGk9hxnbxwf7uRI54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChKik/anaktisipliroforias/blob/main/AnaktisiProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  #mount to drive prwta gia na kanei locate ta file meta"
      ],
      "metadata": {
        "id": "9QKwIt0PftdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BKOwKnn2m7p",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Load ta data se ena morfhs tsv arxeio gia na mporei to colbert na ta diaxeiristei meta\n",
        "\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "\n",
        "null = None\n",
        "true = True\n",
        "false = False\n",
        "\n",
        "\n",
        "#Functions\n",
        "\n",
        "\n",
        "# File paths\n",
        "relevDocs_path = '/content/drive/MyDrive/Anaktisi/Relevant_20'\n",
        "detailedFile_path = '/content/drive/MyDrive/Anaktisi/cfquery_detailed'\n",
        "csvCombined = '/content/drive/MyDrive/Anaktisi/combined_data.csv'\n",
        "\n",
        "# Variables and data storage\n",
        "queriesAnswerDocs = defaultdict(dict)\n",
        "temp_qid = null\n",
        "\n",
        "# Read the data from the files\n",
        "with open(detailedFile_path, 'r') as qfile:\n",
        "    fileElements = qfile.readlines()\n",
        "\n",
        "with open(relevDocs_path, 'r') as relevDocsFile:\n",
        "    relevant_docs = relevDocsFile.readlines()\n",
        "\n",
        "iterator = iter(relevant_docs)\n",
        "\n",
        "############\n",
        "\n",
        "for x in fileElements:\n",
        "    try:\n",
        "        if x.startswith('QN'):\n",
        "            if temp_qid is not null:\n",
        "                queriesAnswerDocs[temp_qid]['RD'] = doc_id  # Yparxei QN to vazei sto antistoixo the doc ID\n",
        "\n",
        "            temp_qid = int(x.split()[1])  # pairnei to query ID me morfh QN 000\n",
        "\n",
        "            line=next(iterator)\n",
        "            doc_id = [int(doc_id) for doc_id in line.split()]\n",
        "\n",
        "        elif x.startswith('NR'):\n",
        "            queriesAnswerDocs[temp_qid] = {'NR': int(x.split()[1]), 'RD': []}  # Initialize ta NR kai RD\n",
        "        elif x.startswith(''):  #agnoei tis grammes poy einai kenes stin arxh\n",
        "          continue\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected line format in fileElements\")\n",
        "\n",
        "    except StopIteration:\n",
        "        print(f\"Error: No more lines in relevant_docs to process for query {temp_qid}\")\n",
        "        break\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"Value error occurred: {ve}\")\n",
        "        exit(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "\n",
        "#vazei sto query 20 poy einai to teleutaio ,ta stoixeia poy toy antistixoyn.\n",
        "if temp_qid is not None:\n",
        "    print(f'Total queries: {temp_qid}')\n",
        "    queriesAnswerDocs[temp_qid]['RD'] = doc_id\n",
        "\n",
        "\n",
        "with open(csvCombined, 'w', newline='') as csv_file:\n",
        "    record_data = csv.writer(csv_file)\n",
        "\n",
        "    # record_data header\n",
        "    record_data.writerow(['Query_ID', 'NR', 'RD'])\n",
        "\n",
        "    # Write data\n",
        "    for query_id, data in queriesAnswerDocs.items():\n",
        "        record_data.writerow([query_id, data['NR'], data['RD']])\n",
        "\n",
        "#printer\n",
        "with open(csvCombined, 'r', newline='') as csv_file:  #csv me tab seperated ara ginetai tsv\n",
        "      csv_reader = csv.reader(csv_file)\n",
        "\n",
        "      for x in csv_reader:\n",
        "          print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n",
        "\n",
        "try: #When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False\n"
      ],
      "metadata": {
        "id": "xWv91WDQtvAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colbert\n",
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ],
      "metadata": {
        "id": "20j8Rnbf0KQD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "docs_path = '/content/drive/MyDrive/Anaktisi/docs'\n",
        "resultDocs_path = '/content/drive/MyDrive/Anaktisi/resultDocs.tsv'\n",
        "\n",
        "resultDocsList=[]\n",
        "\n",
        "with open(resultDocs_path, 'a', encoding='utf-8') as tsv_file:\n",
        "  for filename in os.listdir(docs_path):\n",
        "    full_path = os.path.join(docs_path, filename)  #pairnei to path toy kathe doc ksexwrista\n",
        "    if os.path.isfile(full_path):\n",
        "        try:\n",
        "            with open(full_path, 'r', encoding='utf-8') as temp_file:\n",
        "                document_content = temp_file.read()\n",
        "                resultDocsList.append(document_content)\n",
        "                tsv_file.write(document_content.replace('\\n', ' ') + '\\n')\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"{filename} is not a file,its a directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QaTo_nL30PG7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_path = '/content/drive/MyDrive/Anaktisi/Queries_20'\n",
        "queriesList = []\n",
        "\n",
        "\n",
        "with open(query_path, 'r') as f:\n",
        "    queries = f.read().splitlines()\n",
        "    queriesList.append(queries)\n",
        "\n",
        "numOfDocs = len(os.listdir(docs_path))\n",
        "\n",
        "\n",
        "print(f'Loaded {len(queriesList[0])} queries and {numOfDocs:,} docs')  #bgazei 19,leipei to teleutaio query apo tin Queries_20,den kserw ama einai epithdes etsi.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLTTq6w35ZBR",
        "outputId": "334d7571-3f6b-4a41-d97e-c6b0f848cec3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 19 queries and 1,209 docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "max_id = 2000  #max docs 2000\n",
        "datasplit='dev'\n",
        "dataset='kikidis'\n",
        "\n",
        "index_name = f'{dataset}.{datasplit}.{nbits}bits'\n",
        "\n",
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4)\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=resultDocsList[:max_id], overwrite=True)  #resultDocsList gia ton indexer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OwvZj2rr87og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexer.get_index()\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    searcher = Searcher(index=index_name, collection=resultDocsList)\n"
      ],
      "metadata": {
        "id": "V8AFljG6N1Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "query=\"What is the lipid composition of CF respiratory secretions\"  #mporw na to kanw kai random me map h epilogh kapoiou alla den exei kai poly nohma, to vazw statiko gia test\n",
        "query_num=4\n",
        "\n",
        "docRetriaval=pd.read_csv('/content/drive/MyDrive/Anaktisi/combined_data.csv')\n",
        "\n",
        "queryInfo=docRetriaval[docRetriaval['Query_ID'] == query_num].iloc[0] #tha epistrepsei to prwto antikeimeno stin thesh 0 gia to query ayto.\n",
        "NR=queryInfo['NR']\n",
        "RD=eval(queryInfo['RD'])\n",
        "final_results=searcher.search(query,k=NR)\n",
        "\n",
        "possRelevantDocs = [doc_id for doc_id, doc_rank, doc_score in zip(*final_results)]\n",
        "actual_docs = len(set(possRelevantDocs) & set(RD))\n",
        "\n",
        "if len(possRelevantDocs) > 0:\n",
        "  precision = actual_docs / len(possRelevantDocs)\n",
        "else:\n",
        "  precision=0.0\n",
        "\n",
        "if len(RD) > 0:\n",
        "  recall = actual_docs / len(RD)\n",
        "else:\n",
        "  recall= 0.0\n",
        "\n",
        "print(f\"\\t Rank \\t\\tScore \\t\\t DocumentID\")\n",
        "for doc_id,doc_rank,doc_score in zip(*final_results):\n",
        "    print(f\"\\t [{doc_rank}] \\t\\t {doc_score:.3f} \\t\\t {doc_id}\")\n",
        "\n",
        "print(\"Query Evaluation Measures:\")\n",
        "print(f\"Precision: {precision:.5f}\")\n",
        "print(f\"Recall: {recall:.5f}\")"
      ],
      "metadata": {
        "id": "LrqzERpCOmBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}